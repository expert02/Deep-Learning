{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3362,
     "status": "ok",
     "timestamp": 1611154514910,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "1BTm4gHdSpRP",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#import my_utils as mu\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArsMFnW4SpRZ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Long Short Term Memory (LSTM)\n",
    "\n",
    "* The challenge to address long-term information preservation and short-term input skipping in latent variable models has existed for a long time. \n",
    "* One of the earliest approaches to address this was the LSTM\n",
    "    * More complex than GRU but predates GRU by almost two decades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oURb1ZhCSpRa",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Long Short Term Memory (LSTM)\n",
    "\n",
    "* Main feature is a new structure called the (memory) *cell* which\n",
    "     * has the same shape as the hidden state. \n",
    "     * is a fancy version of a hidden state, engineered to record additional information.\n",
    "* To control a memory cell 3 gates are used: \n",
    "    * The *output* gate is used to read out the entries from the cell.\n",
    "    * The *input* gate is used to read data into the cell.\n",
    "    * The *forget* gate is used to reset the contents of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFurKZ7PSpRb",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Input Gates, Forget Gates, and Output Gates\n",
    "\n",
    "\n",
    "* Similarly to GRU gates, the following figure shows how the values of input, forget and output gates are computed a each time step: \n",
    "  * They are functions of $\\mathbf{X}_t$ and $\\mathbf{H}_{t-1}$. \n",
    "  * The function output is given by a fully connected layer with a sigmoid as its activation function.\n",
    "\n",
    "![Calculation of input, forget, and output gates in an LSTM. ](img/lstm_0.svg) \n",
    "\n",
    "<!-- ![ Calculation of input, forget, and output gates in an LSTM. ](https://drive.google.com/uc?export=view&id=1ntIt3neQUkhBCTII3Eyvw97i55nIMh4M) -->\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xj9NflnYSpRc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Assume, for a given time step $t$, the minibatch input is $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ (number of examples: $n$, number of inputs: $d$) and the hidden state of the last time step is $\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$ (number of hidden states: $h$). \n",
    "\n",
    "* Then, the input gate $\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}$, the forget gate $\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}$ and the output gate $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}$ are computed as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{I}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i),\\\\\n",
    "\\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f),\\\\\n",
    "\\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* $\\mathbf{W}_{xi}, \\mathbf{W}_{xf}, \\mathbf{W}_{xo} \\in \\mathbb{R}^{d \\times h}$ and $\\mathbf{W}_{hi}, \\mathbf{W}_{hf}, \\mathbf{W}_{ho} \\in \\mathbb{R}^{h \\times h}$ are weight parameters and $\\mathbf{b}_i, \\mathbf{b}_f, \\mathbf{b}_o \\in \\mathbb{R}^{1 \\times h}$ are bias parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFQxUetpSpRd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "#  Candidate Memory Cell\n",
    "\n",
    "* Akin to candidate hidden state in GRU.\n",
    "* The *candidate* memory cell $\\tilde{\\mathbf{C}}_t \\in \\mathbb{R}^{n \\times h}$ at time step $t$ is computed from:\n",
    "$$\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c)$$\n",
    "where $\\mathbf{W}_{xc} \\in \\mathbb{R}^{d \\times h}$ and $\\mathbf{W}_{hc} \\in \\mathbb{R}^{h \\times h}$ are weight parameters and $\\mathbf{b}_c \\in \\mathbb{R}^{1 \\times h}$ is a bias parameter.\n",
    "\n",
    "\n",
    "![Computation of candidate memory cells in LSTM. ](img/lstm_1.svg) \n",
    "\n",
    "\n",
    "<!-- ![ Computation of candidate memory cells in LSTM. ](https://drive.google.com/uc?export=view&id=1Q5UvabukTAZ2rzxKcMwD5HQ7ZpMchCED)  --> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5q98ZnvxSpRd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Memory Cell\n",
    "\n",
    "* In GRUs, we had a single mechanism to govern input and forgetting. \n",
    "* In LSTMs we have two parameters: \n",
    "  * input gate $\\mathbf{I}_t$ defines how much we take new data into account from $\\tilde{\\mathbf{C}}_t$ \n",
    "  * forget gate $\\mathbf{F}_t$ defines how much of the old memory cell content $\\mathbf{C}_{t-1} \\in \\mathbb{R}^{n \\times h}$ is retained. \n",
    "* Put together we have the following update equation:\n",
    "\n",
    "$$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t.$$\n",
    "\n",
    "\n",
    "![Computation of memory cells in an LSTM. Here, the multiplication is carried out elementwise. ](img/lstm_2.svg)\n",
    "\n",
    "<!-- ![Computation of memory cells in an LSTM. Here, the multiplication is carried out elementwise. ](https://drive.google.com/uc?export=view&id=16ukauP-5xZO1fHF9LzeRo6_eUys2VI8Y) -->\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3qiLTVVSpRe",
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hidden States\n",
    "\n",
    "* Last, we need to define how to compute the hidden state $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$. \n",
    "* In LSTM it is simply a gated version of the $\\tanh$ of the memory cell. \n",
    "* This is where the output gate comes into play:\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t).$$\n",
    "\n",
    "![Computation of the hidden state. Multiplication is elementwise. ](img/lstm_3.svg)\n",
    "\n",
    "<!-- ![Computation of the hidden state. Multiplication is elementwise. ](https://drive.google.com/uc?export=view&id=16e8COQxcx-X4avmW6b46R8-Ne5D0SXqj) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HP3MabJSpRn",
    "origin_pos": 17,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Summary\n",
    "\n",
    "* LSTMs have three types of gates: input gates, forget gates, and output gates which control the flow of information.\n",
    "* The hidden layer output of LSTM includes hidden states and memory cells. Only hidden states are passed into the output layer. Memory cells are entirely internal.\n",
    "* LSTMs can cope with vanishing and exploding gradients.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "Long_Short_Term_Memory.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
